{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bn_cG3mHM0dt"
   },
   "source": [
    "## Assignment Information\n",
    "\n",
    "| Field | Details |\n",
    "|-------|---------|\n",
    "| **Name** | Pros Loung |\n",
    "| **Course** | AAI-521 Applied Computer Vision for AI |\n",
    "| **Assignment** | 4.1 - Advanced Vision Problem|\n",
    "| **GitHub Repository** | https://github.com/ploung/AAI_521_ComputerVision_Module4.git |\n",
    "\n",
    "---\n",
    "\n",
    "### Assignment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1- Feature extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimutils\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpatches\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cv2_imshow\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display, Javascript\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moutput\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m eval_js\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#Start with uploading the required images on Google drive\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imutils\n",
    "from google.colab.patches import cv2_imshow\n",
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to Google Colab's cv2_imshow for local environments\n",
    "def cv2_imshow(image):\n",
    "    \"\"\"Display image using matplotlib instead of Google Colab's cv2_imshow\"\"\"\n",
    "    if len(image.shape) == 3:  # Color image\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    else:  # Grayscale image\n",
    "        plt.imshow(image, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the missing imutils package\n",
    "!pip install imutils\n",
    "\n",
    "# Load and display the image\n",
    "img_rgb = cv2.imread('/content/Assignment4_pic1.jpg')\n",
    "img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(img_rgb, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "f.add_subplot(1, 2, 2)\n",
    "plt.imshow(img_gray, cmap='gray')\n",
    "plt.title('Grayscale Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpOm_Ji_Yn1o"
   },
   "outputs": [],
   "source": [
    "# Read the original image pic1 and create a regular plus a gray version of it.\n",
    "# The result should be similar to what you see in instrcutions\n",
    "f = plt.figure()\n",
    "f.add_subplot(1, 2, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kByVE9hkNJ5p"
   },
   "outputs": [],
   "source": [
    "#a- SIFT\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# Find detect and compute function and apply it on top of sift\n",
    "original_keypoints, original_descriptor = sift.detectAndCompute( img_gray, None)\n",
    "keypoints_without_size = np.copy(img_rgb)\n",
    "# You need some more codes\n",
    "\n",
    "# Draw the result here\n",
    "result1 = cv2.drawKeypoints(img_rgb, original_keypoints, keypoints_with_size,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
    "result2 = cv2.drawKeypoints(img_rgb, original_keypoints,keypoints_without_size, color = (0, 255, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yy4PMuTBqA7t"
   },
   "outputs": [],
   "source": [
    "# Explain your understanding here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCOIfTH1NJ8P"
   },
   "outputs": [],
   "source": [
    "# b- FAST\n",
    "fast = cv2.FastFeatureDetector_create()\n",
    "# Find detect and compute function and apply it on top of fast\n",
    "keypoints_with_nonmax = fast.detect(#put something here- maybe use above approach)\n",
    "fast.setNonmaxSuppression(False)\n",
    "image_with_nonmax = np.copy(img_rgb)\n",
    "# You need some more code here\n",
    "\n",
    "# Draw the result\n",
    "# Use above approach to create result 1 and result 2 and draw them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ADBAzhxqEuM"
   },
   "outputs": [],
   "source": [
    "# Explain your understanding here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vptTdxxzNJ-i"
   },
   "outputs": [],
   "source": [
    "# c- ORB\n",
    "orb = cv2.ORB_create()\n",
    "original_keypoints, original_descriptor = orb.detectAndCompute(#put something here- maybe use above approach)\n",
    "query_keypoints, query_descriptor = orb.detectAndCompute(#put something here- maybe use above approach)\n",
    "keypoints_without_size = np.copy(img_rgb)\n",
    "keypoints_with_size = np.copy(img_rgb)\n",
    "\n",
    "# Draw the result\n",
    "# Use above approach to create result 1 and result 2 and draw them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nDFtEQdqIdu"
   },
   "outputs": [],
   "source": [
    "# Explain your understanding here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "skWZjpMuNKAy",
    "outputId": "772904b8-5ebf-4bcb-a132-9d2690d35843"
   },
   "outputs": [],
   "source": [
    "#d- Finding the matching points\n",
    "brute_force = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)\n",
    "matches = brute_force.match(original_descriptor, query_descriptor)\n",
    "matches = sorted(matches, key = lambda x : x.distance)\n",
    "result = cv2.drawMatches(img_rgb, original_keypoints, img_gray, query_keypoints, matches, img_gray, flags = 2)\n",
    "plt.imshow(result)\n",
    "print(\"The number of matching keypoints between the original and the query image is {}\\n\".format(len(matches)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY85Tsn0qLQF"
   },
   "outputs": [],
   "source": [
    "# Explain your understanding here. What is the goal of this matching point?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtDzJp7o-5iO"
   },
   "source": [
    "**Part 2- Face recognition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jh7BKMcNKDI"
   },
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(\"/content/haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(\"/content/haarcascade_eye.xml\")\n",
    "\n",
    "img = cv2.imread(\"/content/Assignment4_pic2.jpg\")\n",
    "gray = #Cretae the gray version of image using openCV\n",
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    img1 = cv2.rectangle(img,(x, y),(x + w, y + h),(255,255,0),2)\n",
    "    roi_gray = gray[y: y + h, x: x + w]\n",
    "    roi_color = img[y: y + h, x: x + w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex, ey, ew, eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex, ey), (ex + ew, ey + eh), (0,255,0),2)\n",
    "\n",
    "cv2_imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clYu5ua5NKH_"
   },
   "outputs": [],
   "source": [
    "#b- Working on webcam\n",
    "#Function to activate your laptop cam. This function is comp;ete and you don't need to do anything.\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LktDbjf6NKKU"
   },
   "outputs": [],
   "source": [
    "# Here we call the function and open the camera\n",
    "# Call function and open the camera\n",
    "image_file =\n",
    "\n",
    "#Show the read image\n",
    "image = #read the image and show it here\n",
    "\n",
    "\n",
    "#Repeat the above code to detect the face and eyes in your image\n",
    "# Here you should see your image while the face and eyes are detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wL2nEtNr1_gq"
   },
   "source": [
    "**Part 3- Parameter detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q38jlIPgNKPZ"
   },
   "outputs": [],
   "source": [
    "lowerBound=np.array([170,100,80])\n",
    "upperBound=np.array([180,256,256])\n",
    "\n",
    "#cam= cv2.VideoCapture(0)\n",
    "kernelOpen=np.ones((5,5))\n",
    "kernelClose=np.ones((20,20))\n",
    "\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "img=cv2.imread('/content/apple.jpg')\n",
    "\n",
    "#convert BGR to HSV\n",
    "imgHSV=\n",
    "# Show this image\n",
    "\n",
    "# create the Mask\n",
    "mask=cv2.inRange(imgHSV,lowerBound,upperBound)\n",
    "#morphology\n",
    "maskOpen=cv2.morphologyEx(mask,cv2.MORPH_OPEN,kernelOpen)\n",
    "maskClose=cv2.morphologyEx(maskOpen,cv2.MORPH_CLOSE,kernelClose)\n",
    "\n",
    "maskFinal=maskClose\n",
    "conts,h=cv2.findContours(maskFinal.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "cv2.drawContours(img,conts,-1,(255,0,0),1)\n",
    "for i in range(len(conts)):\n",
    "    x,y,w,h=cv2.boundingRect(conts[i])\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255), 2)\n",
    "    cv2.putText(img, str(i+1),(x,y+h),cv2.FONT_HERSHEY_SIMPLEX,0.7,(0,255,0))\n",
    "\n",
    "cv2_imshow(maskClose)\n",
    "cv2_imshow(maskOpen)\n",
    "cv2_imshow(mask)\n",
    "cv2_imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iVRX68CixHhX"
   },
   "outputs": [],
   "source": [
    "print( 'There were ',#???# ,' apples in this picture') #Print number of apples"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
